{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report contains, in addition to the code provided by the lecturer, my own code changes\n",
    "and additions, and written answers to answer coursework 9 in Machine Learning and Statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries needed\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Seed random number generator\n",
    "np.random.seed(123)\n",
    "\n",
    "# Helper functions for polynomials\n",
    "def powers( X, n ):\n",
    "    \"\"\" Returns an array of the powers of the elements of X up to the nth power \"\"\"\n",
    "    return np.power(np.expand_dims(X, axis=-1), [np.arange(n)])\n",
    "\n",
    "def polynomial( X, poly_coeff ):\n",
    "    \"\"\" Returns the value of a polynomial at x with given coefficients \"\"\"\n",
    "    deg = np.shape(poly_coeff)[-1]\n",
    "    return np.dot(powers(X, deg), np.transpose(poly_coeff), )\n",
    "\n",
    "def fit_polynomial( X, y, n ):\n",
    "    \"\"\" Returns the coefficients of the n-degree polynomial fit to (X, y) \"\"\"\n",
    "    X_pwrs = powers(X, n+1)\n",
    "    # Do linear least squares fit\n",
    "    coeff, _, _, _ = np.linalg.lstsq(X_pwrs, y, rcond=None)\n",
    "    return coeff\n",
    "\n",
    "def degree( poly_coeff ):\n",
    "    \"\"\" Returns the degree of a polynomial from its coefficients \"\"\"\n",
    "    return len(poly_coeff)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSS, AIC, and BIC scores\n",
    "# You should modify the functions aic_loss and bic_loss to compute\n",
    "# the AIC, and BIC, penalties given a dataset X, y, and learned\n",
    "# polynomial model with coefficients learned_coeff.\n",
    "\n",
    "def rss_loss(X, y, learned_coeff):\n",
    "    \"\"\" Computes the residual sum of squares loss for a given polynomial on \n",
    "    the given data \"\"\"\n",
    "    pred_y = polynomial(X, learned_coeff)\n",
    "    return np.sum(np.square(pred_y - y))\n",
    "    \n",
    "def aic_loss(X, y, learned_coeff):\n",
    "    \"\"\" Computes the Akaikeâ€™s Information Criterion for a given polynomial on \n",
    "    the given data \"\"\"\n",
    "    # You will need to modify this function to return the correct result\n",
    "    RSS = rss_loss(X,y,learned_coeff) \n",
    "    aic = (1/len(X))*(RSS+2*len(learned_coeff))\n",
    "    return aic\n",
    "    \n",
    "def bic_loss(X, y, learned_coeff):\n",
    "    \"\"\" Computes the Bayesian Information Criterion for a given polynomial on \n",
    "    the given data \"\"\"\n",
    "    # You will need to modify this function to return the correct result\n",
    "    RSS = rss_loss(X,y,learned_coeff) \n",
    "    bic = (1/len(X))*(RSS+len(learned_coeff)*np.log(len(X)))\n",
    "    return bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted a degree 5 polynomial with RSS loss: 15.34, AIC loss: 1.82, BIC loss: 2.11 to 15 datapoints.\n",
      "[-1.09065806  2.02523857 -0.33298202  0.48379912 -0.02799822  0.00711476]\n"
     ]
    }
   ],
   "source": [
    "# Using AIC/BIC to compare models\n",
    "# You should see how the AIC and BIC scores compare for different\n",
    "# models by changing the model degree (given by the variable n).\n",
    "\n",
    "# Load data\n",
    "X, y = np.load(\"./class9_generated_data.npy\")\n",
    "\n",
    "#Plot the data\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X,y)\n",
    "plt.ylim(y.min()-1., y.max()+1.)\n",
    "\n",
    "# Make sequence of x-values for plotting\n",
    "axis_X = np.arange(-4.0, 4.0, 0.2)\n",
    "\n",
    "# Fit a polynomial\n",
    "n = 5\n",
    "learned_coefficients = fit_polynomial( X, y, n )\n",
    "\n",
    "#Plot the learned polynomial\n",
    "poly_y = polynomial(axis_X, learned_coefficients)\n",
    "plt.plot(axis_X, poly_y, )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "num_points = np.shape(X)[0]\n",
    "rss = rss_loss(X, y, learned_coefficients)\n",
    "aic = aic_loss(X, y, learned_coefficients)\n",
    "bic = bic_loss(X, y, learned_coefficients)\n",
    "str_ = \"Fitted a degree {:d} polynomial with \".format(degree(learned_coefficients))\n",
    "str_ += \"RSS loss: {:.2f}, AIC loss: {:.2f}, BIC loss: {:.2f} \".format(rss, aic, bic)\n",
    "print(str_ + \"to {:d} datapoints.\".format(num_points))\n",
    "\n",
    "print(learned_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the functions aic_loss and bic_loss for calculating the AIC and the\n",
    "BIC scores for a learned or trained polynomial model. In your implementation, you can use the\n",
    "provided function to calculate the degree of a given polynomial model. Run the code and record\n",
    "the RSS, AIC, and BIC scores on the given data set. Repeat this for polynomial models of degrees 1, 3,\n",
    "and 5. Report all the scores for polynomial models of different degrees. With the results achieved,\n",
    "comment on which polynomial degree has been used to generate the data set. <br>\n",
    "Answer: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The completed functions for AIC and BIC loss can be found in the code above. Report for polynomial models of different degrees: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Degree 1 polynomial (n = 1): Fitted a degree 1 polynomial with RSS loss: 100.73, AIC loss: 6.98, BIC loss: 7.08 to 15 datapoints. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Degree 3 polynomial (n = 3): Fitted a degree 3 polynomial with RSS loss: 15.57, AIC loss: 1.57, BIC loss: 1.76 to 15 datapoints. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Degree 5 polynomial (n = 5): Fitted a degree 5 polynomial with RSS loss: 15.34, AIC loss: 1.82, BIC loss: 2.11 to 15 datapoints. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the polynomial of degree 3 gives the smallest values of BIC and AIC loss, thus degree 3 is the degree with which the polynomial was generated. Tested with n = 2 and n = 4 as well just to be sure that these were not better. <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for linear models\n",
    "def linear ( X, coefficients ):\n",
    "    return np.dot(X, np.transpose(coefficients), )\n",
    "\n",
    "def fit_linear( X, y, features=None ):\n",
    "    \"\"\" \n",
    "        Returns the coefficients of a linear model fit to X,y.\n",
    "        If features is a list of integers, then fit will ignore\n",
    "        any features whose index is not in the list.\n",
    "        ( Returned coefficients for these features will be set\n",
    "        to 0. )\n",
    "    \"\"\"\n",
    "    if features is not None:\n",
    "        # Make a mask\n",
    "        tot_num_features = np.shape(X)[-1]\n",
    "        mask = np.zeros((1,tot_num_features))\n",
    "        mask[0,features] = 1.\n",
    "        # Zero out all irrellevant features\n",
    "        X = X * mask\n",
    "    \n",
    "    # Do linear least squares fit\n",
    "    coeff, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 160 entries.\n",
      "\n",
      "Performing forwards stepwise feature selection using AIC as criteria...\n",
      "Round 1, selected feature 1 with score 0.754\n",
      "Round 2, selected feature 0 with score 0.530\n",
      "Round 3, selected feature 3 with score 0.490\n",
      "Round 4, selected feature 2 with score 0.498\n",
      "Round 5, selected feature 9 with score 0.506\n",
      "Round 6, selected feature 12 with score 0.516\n",
      "Round 7, selected feature 13 with score 0.517\n",
      "Round 8, selected feature 4 with score 0.528\n",
      "Round 9, selected feature 10 with score 0.539\n",
      "Round 10, selected feature 5 with score 0.550\n",
      "Round 11, selected feature 11 with score 0.560\n",
      "Round 12, selected feature 6 with score 0.572\n",
      "Round 13, selected feature 7 with score 0.584\n",
      "Round 14, selected feature 14 with score 0.596\n",
      "Round 15, selected feature 8 with score 0.608\n",
      "Best features were [1, 0, 3] (3 total) with score 0.490\n"
     ]
    }
   ],
   "source": [
    "# AIC and BIC for feature selection\n",
    "# The following is an implementation of forward feature selection.\n",
    "# However, as a criterion it uses the Mean Square Error (MSE) score\n",
    "# ( = RSS / num_examples). This is largely useless as adding more\n",
    "# features will always give a lower MSE score.\n",
    "# You should modify the code below by calculating the AIC penalty for\n",
    "# the learned linear model each iteration and setting the score variable\n",
    "# to this value.\n",
    "\n",
    "# Load data\n",
    "data = []\n",
    "continuous_features = [ 0, 1, 9, 10, 11, 12, 13, 16, 18, 19, 20, 21, 22, 23, 24, 25 ]\n",
    "\n",
    "# Original data is from https://archive.ics.uci.edu/ml/datasets/automobile\n",
    "with open('./automobile.csv', 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\\\"')\n",
    "    for row in csvreader:\n",
    "        try:\n",
    "            # Get all continuous rows\n",
    "            data.append([float(row[i]) for i in continuous_features])\n",
    "        except:\n",
    "            continue # skip this row since data-processing failed\n",
    "\n",
    "data = np.array(data)\n",
    "y = data[:, 0] # target is first value\n",
    "X = data[:, 1:] # training data is the rest\n",
    "\n",
    "# Normalize the data to zero mean and unit std\n",
    "X = (X - np.mean(X, axis=0, keepdims=True)) / np.std(X, axis=0, keepdims=True)\n",
    "y = (y - np.mean(y)) / np.std(y)\n",
    "\n",
    "print(\"Successfully loaded {:d} entries.\\n\".format(len(X)))\n",
    "    \n",
    "\n",
    "# Forwards stepwise feature selection\n",
    "tot_num_features = np.shape(X)[-1]\n",
    "all_features = range(tot_num_features)\n",
    "curr_features = []\n",
    "\n",
    "best_score = 1e9\n",
    "best_features = None\n",
    "\n",
    "print(\"Performing forwards stepwise feature selection using AIC as criteria...\")\n",
    "# While we have remaining features...\n",
    "while len(curr_features) != tot_num_features:\n",
    "    # Get remaining features \n",
    "    candidate_features = [ f for f in all_features if f not in curr_features ]\n",
    "    \n",
    "    best_score_this_round = 1e9\n",
    "    best_feature_this_round = None\n",
    "    for f in candidate_features:\n",
    "        test_features = curr_features + [f]\n",
    "        learned_coefficients = fit_linear( X, y, test_features )\n",
    "        \n",
    "        y_pred = linear(X, learned_coefficients)\n",
    "        RSS_loss = np.sum(np.square(y_pred - y))\n",
    "        AIC_loss = (1/len(X))*(RSS_loss+(2*(len(test_features)+1)))\n",
    "        score = AIC_loss\n",
    "        \n",
    "        # Remember, lower score is better\n",
    "        if score < best_score_this_round:\n",
    "            best_score_this_round = score\n",
    "            best_feature_this_round = f\n",
    "            \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_features = test_features\n",
    "            \n",
    "    # Set current features to best features from round\n",
    "    curr_features = curr_features + [best_feature_this_round]\n",
    "    print(\"Round {}, selected feature {:d} with score {:.3f}\".format(len(curr_features), \n",
    "                                    best_feature_this_round, best_score_this_round ))\n",
    "            \n",
    "print(\"Best features were {} ({:d} total) with score {:.3f}\".format(best_features, \n",
    "                                                len(best_features), best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Modify the provided forward feature selection routine to use the AIC score instead\n",
    "of the RSS as its evaluation criterion. Run your modified code on the Automobile data set to select\n",
    "a best feature subset for the insurance category prediction of a vehicle. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See modified code above. The best feature subet, as the output above suggests, is 1 , 0 og 3, with an AIC score of 0.490."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 160 entries.\n",
      "\n",
      "Performing backward stepwise feature selection using AIC as criteria...\n",
      "Round 14, selected feature 8 to be removed, with score 0.596\n",
      "Round 13, selected feature 14 to be removed, with score 0.584\n",
      "Round 12, selected feature 2 to be removed, with score 0.571\n",
      "Round 11, selected feature 7 to be removed, with score 0.559\n",
      "Round 10, selected feature 6 to be removed, with score 0.548\n",
      "Round 9, selected feature 11 to be removed, with score 0.538\n",
      "Round 8, selected feature 5 to be removed, with score 0.527\n",
      "Round 7, selected feature 10 to be removed, with score 0.516\n",
      "Round 6, selected feature 4 to be removed, with score 0.505\n",
      "Round 5, selected feature 9 to be removed, with score 0.502\n",
      "Round 4, selected feature 13 to be removed, with score 0.502\n",
      "Round 3, selected feature 12 to be removed, with score 0.490\n",
      "Round 2, selected feature 3 to be removed, with score 0.530\n",
      "Round 1, selected feature 0 to be removed, with score 0.754\n",
      "Round 0, selected feature 1 to be removed, with score 1.012\n",
      "Best features were [0, 1, 3] (3 total) with score 0.490\n"
     ]
    }
   ],
   "source": [
    "# Backwards feature selection\n",
    "# Copy the code above and modify it to implement backwards feature\n",
    "# selection using the AIC criterion as a scoring function.\n",
    "\n",
    "# Load data\n",
    "data = []\n",
    "continuous_features = [ 0, 1, 9, 10, 11, 12, 13, \n",
    "                       16, 18, 19, 20, 21, 22, 23, 24, 25 ]\n",
    "\n",
    "# Original data is from https://archive.ics.uci.edu/ml/datasets/automobile\n",
    "with open('./automobile.csv', 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\\\"')\n",
    "    for row in csvreader:\n",
    "        try:\n",
    "            # Get all continuous rows\n",
    "            data.append([float(row[i]) for i in continuous_features])\n",
    "        except:\n",
    "            continue # skip this row since data-processing failed\n",
    "\n",
    "data = np.array(data)\n",
    "y = data[:, 0] # target is first value\n",
    "X = data[:, 1:] # training data is the rest\n",
    "\n",
    "# Normalize the data to zero mean and unit std\n",
    "X = (X - np.mean(X, axis=0, keepdims=True)) / np.std(X, axis=0, \n",
    "                                                     keepdims=True)\n",
    "y = (y - np.mean(y)) / np.std(y)\n",
    "\n",
    "print(\"Successfully loaded {:d} entries.\\n\".format(len(X)))\n",
    "    \n",
    "#Backward stepwise feature selection\n",
    "\n",
    "#start out by saying that we start out with all the features\n",
    "tot_num_features = np.shape(X)[-1]\n",
    "all_features = range(tot_num_features)\n",
    "curr_features = list(range(tot_num_features))\n",
    "\n",
    "#calculate the AIC-score for the full model and start out with that\n",
    "#as the best score and model\n",
    "learned_coefficients = fit_linear( X, y, curr_features )\n",
    "y_pred = linear(X, learned_coefficients)\n",
    "RSS_loss = np.sum(np.square(y_pred - y))\n",
    "AIC_score_start = (1/len(X))*(RSS_loss+(2*(len(curr_features)+1)))\n",
    "\n",
    "best_score = AIC_score_start\n",
    "best_features = curr_features\n",
    "\n",
    "print(\"Performing backward stepwise feature selection using AIC as criteria...\")\n",
    "# While we have remaining features...\n",
    "while len(curr_features) > 0:   \n",
    "    #need to keep track of the best score and the feature which\n",
    "    #gave the best result when removed\n",
    "    best_score_this_round = 1e9\n",
    "    worst_feature_this_round = None\n",
    "    \n",
    "    for f in range(len(curr_features)):  \n",
    "        #for each loop remove one of the features\n",
    "        i = curr_features[f]\n",
    "        test_features = curr_features\n",
    "        test_features = list(filter(lambda x:x != i,test_features))\n",
    "        \n",
    "        #test model without selected feature\n",
    "        learned_coefficients = fit_linear( X, y, test_features )\n",
    "        y_pred = linear(X, learned_coefficients)\n",
    "        RSS_loss = np.sum(np.square(y_pred - y))\n",
    "        AIC_score = (1/len(X))*(RSS_loss+(2*(len(test_features)+1)))\n",
    "        \n",
    "        # Remember, lower score is better\n",
    "        if AIC_score < best_score_this_round:\n",
    "            best_score_this_round = AIC_score\n",
    "            worst_feature_this_round = i\n",
    "            \n",
    "        if AIC_score < best_score:\n",
    "            best_score = AIC_score\n",
    "            best_features = test_features\n",
    "\n",
    "    # Set current features to best features from round\n",
    "    #by removing the \"worst\" feature from this round\n",
    "    curr_features = list(filter(lambda x:x != worst_feature_this_round,curr_features))\n",
    "    print(\"Round {}, selected feature {:d} to be removed, with score {:.3f}\"\\\n",
    "          .format(len(curr_features), worst_feature_this_round, best_score_this_round ))\n",
    "            \n",
    "print(\"Best features were {} ({:d} total) with score {:.3f}\".format(best_features, \n",
    "                                                    len(best_features), best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the backward stepwise feature selection routine. Run your backward\n",
    "selection code on the Automobile data set to select a best feature subset for the insurance category\n",
    "prediction of a vehicle. Compare the results of running backwards feature selection to running\n",
    "forwards feature selection. With the results and your justification, comment on whether there is a\n",
    "difference between two feature selection methods in terms of this data set.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the above code for my implementation of backward feature selection. We see by the output that the backward stepwise feature selections selects the same feature subset as the forward routine. So for this dataset, there is no difference in their conclusion. However, if one looks at the order in which features are added/taken away - there is a difference. Still, the conlusion is the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
